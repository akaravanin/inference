# Inference Server â€” Architecture & Design

## Architecture

```
React Frontend (port 3000)
    â†“ GraphQL mutation / query (HTTP)
Rust API Server (port 8080)   â† axum + async-graphql
    â†“ HTTP POST /infer  (internal Docker network)
Python Model Worker (port 8001, internal only)
    â†“  [optional] DuckDuckGo web search â†’ context injection
    â†“ HuggingFace transformers
GPU / CPU inference
```

## Design Decisions

### Why Rust for the API layer?
The Rust server does **not** load the model. It handles:
- GraphQL schema, request validation, auth
- Queuing, rate limiting, streaming coordination
- Connection multiplexing to the Python worker

Axum + async-graphql give near-zero overhead. Rust's ownership model makes
concurrent request handling safe without a GIL.

### Why Python for the model worker?
PyTorch and HuggingFace transformers are Python-native. Running LLaMA inference
from Rust (via tch-rs or ONNX) eats a weekend on compatibility issues alone.
The worker is **internal-only** â€” never exposed outside the Docker network.

### Why GraphQL?
- `mutation infer(prompt, webSearch)` for inference requests
- `query health` for readiness checks
- Subscriptions wired for streaming (SSE / WebSocket) when you need it
- Self-documenting schema; introspection works out of the box

### Web Search (RAG)
When `webSearch: true` is passed, the Python worker:
1. Queries DuckDuckGo for the user's prompt (top 4 results)
2. Injects results as a system message before the model prompt
3. Instructs the model to answer from the live results and ignore its training cutoff

Library: `ddgs` (the renamed `duckduckgo-search`). No API key required.

## Project Structure

```
inference-server/
â”œâ”€â”€ CLAUDE.md
â”œâ”€â”€ README.md
â”œâ”€â”€ .env.example              â† copy to .env, add HF_TOKEN
â”œâ”€â”€ .gitignore
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ start.sh              # build + start (detached)
â”‚   â”œâ”€â”€ stop.sh               # stop and remove containers
â”‚   â”œâ”€â”€ restart.sh            # stop â†’ rebuild â†’ start
â”‚   â””â”€â”€ install-nvidia.sh     # NVIDIA driver + container toolkit
â”œâ”€â”€ rust-server/
â”‚   â”œâ”€â”€ Dockerfile            # multi-stage: rust:1-slim â†’ debian:bookworm-slim
â”‚   â”œâ”€â”€ Cargo.toml            # axum, async-graphql, reqwest (rustls), tower-http
â”‚   â””â”€â”€ src/main.rs           # GraphQL schema: infer(prompt, webSearch?) + health
â”œâ”€â”€ python-worker/
â”‚   â”œâ”€â”€ Dockerfile            # python:3.11-slim + torch cu121
â”‚   â”œâ”€â”€ requirements.txt      # fastapi, transformers, peft, bitsandbytes, ddgs
â”‚   â””â”€â”€ worker.py             # model load, DDG search, /infer endpoint
â””â”€â”€ frontend/
    â”œâ”€â”€ Dockerfile
    â”œâ”€â”€ package.json          # React, Vite, Apollo Client, GraphiQL v3
    â”œâ”€â”€ vite.config.ts
    â”œâ”€â”€ tsconfig.json
    â”œâ”€â”€ index.html
    â””â”€â”€ src/
        â”œâ”€â”€ main.tsx          # Apollo provider â†’ VITE_GRAPHQL_URL
        â””â”€â”€ App.tsx           # Tab 1: chat + web search toggle | Tab 2: GraphiQL
```

## Services

| Service | Port | Tech | Responsibility |
|---------|------|------|----------------|
| `rust-server` | 8080 (public) | Axum + async-graphql | GraphQL API, routing |
| `python-worker` | 8001 (internal) | FastAPI + transformers + ddgs | Model inference + web search |
| `frontend` | 3000 (public) | React + Vite + Apollo + GraphiQL | Chat UI + schema explorer |

## Quick Start

```bash
cp .env.example .env          # add HF_TOKEN
./scripts/start.sh
```

URLs once running:
- **Frontend + Chat** â†’ http://localhost:3000
- **GraphQL Explorer** â†’ http://localhost:3000 (second tab)
- **GraphQL endpoint** â†’ http://localhost:8080/graphql

## Environment Variables

| Variable | Default | Notes |
|----------|---------|-------|
| `HF_TOKEN` | â€” | Required for gated models (Llama etc.) |
| `MODEL_ID` | `meta-llama/Llama-3.2-3B-Instruct` | Any HF causal LM |
| `LOAD_IN_4BIT` | `false` | Enable for 7B+ models on â‰¤12 GB VRAM |

## GPU Support

The `python-worker` service has the NVIDIA `deploy` block active in
`docker-compose.yml`. Requires the NVIDIA Container Toolkit on the host:

```bash
./scripts/install-nvidia.sh   # then reboot
```

## Frontend Tabs

**Chat** â€” bubble-style messages. Web search toggle in the input bar (ğŸŒ Off/On).
When on, the user message is labelled `ğŸŒ web search` and the loading indicator
shows `ğŸŒ searchingâ€¦`.

**GraphQL Explorer** â€” embedded GraphiQL v3. Schema introspection, query/mutation
builder, variables panel, history, docs sidebar. Points at `rust-server:8080`.

## Extending

- **Streaming**: Add a `Subscription` in `rust-server/src/main.rs`, switch Apollo
  Client to a WebSocket link in `frontend/src/main.tsx`
- **Queuing**: Add Redis to compose; Rust enqueues, Python dequeues
- **Multiple models**: Add a `model: String` arg to the GraphQL mutation, route
  in `worker.py`
- **Auth**: Add a Tower middleware layer in Rust before the GraphQL handler
- **LoRA fine-tuning**: `peft` is already installed in the worker image:

```python
from peft import get_peft_model, LoraConfig
config = LoraConfig(r=16, lora_alpha=32, target_modules=["q_proj", "v_proj"])
model = get_peft_model(model, config)
```
