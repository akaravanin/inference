services:

  rust-server:
    build: ./rust-server
    ports:
      - "8080:8080"
    environment:
      - WORKER_URL=http://python-worker:8001
    depends_on:
      - python-worker
    restart: unless-stopped

  python-worker:
    build: ./python-worker
    expose:
      - "8001"
    environment:
      - HF_TOKEN=${HF_TOKEN}
      - MODEL_ID=${MODEL_ID:-meta-llama/Llama-3.2-3B-Instruct}
      - LOAD_IN_4BIT=${LOAD_IN_4BIT:-false}  # 3B fits in float16 on 12GB VRAM
      - ADAPTER_DIR=/adapters
    volumes:
      - hf-cache:/root/.cache/huggingface   # persist downloaded model weights
      - adapters:/adapters                  # persist trained LoRA adapters
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped

  frontend:
    build: ./frontend
    ports:
      - "3000:5173"
    environment:
      - VITE_GRAPHQL_URL=http://localhost:8080/graphql
      - VITE_GRAPHQL_WS_URL=ws://localhost:8080/graphql/ws
    depends_on:
      - rust-server
    restart: unless-stopped

volumes:
  hf-cache:
  adapters:
